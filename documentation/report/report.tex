%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Journal Article
% LaTeX Template
% Version 1.4 (15/5/16)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Frits Wenneker (http://www.howtotex.com) with extensive modifications by
% Vel (vel@LaTeXTemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[twoside,twocolumn]{article}

\usepackage{blindtext} % Package to generate dummy text throughout this template 

\usepackage[sc]{mathpazo} % Use the Palatino font
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\linespread{1.05} % Line spacing - Palatino needs more space between lines
\usepackage{microtype} % Slightly tweak font spacing for aesthetics

\usepackage{graphicx}
\graphicspath{ {./asset/} }

\usepackage[english]{babel} % Language hyphenation and typographical rules

\usepackage[hmarginratio=1:1,top=32mm,columnsep=20pt]{geometry} % Document margins
\usepackage[hang, small,labelfont=bf,up,textfont=it,up]{caption} % Custom captions under/above floats in tables or figures
\usepackage{booktabs} % Horizontal rules in tables

\usepackage{lettrine} % The lettrine is the first enlarged letter at the beginning of the text

\usepackage{enumitem} % Customized lists
\setlist[itemize]{noitemsep} % Make itemize lists more compact

\usepackage{abstract} % Allows abstract customization
\renewcommand{\abstractnamefont}{\normalfont\bfseries} % Set the "Abstract" text to bold
\renewcommand{\abstracttextfont}{\normalfont\small\itshape} % Set the abstract itself to small italic text

\usepackage{titlesec} % Allows customization of titles
\renewcommand\thesection{\Roman{section}} % Roman numerals for the sections
\renewcommand\thesubsection{\roman{subsection}} % roman numerals for subsections
\titleformat{\section}[block]{\large\scshape\centering}{\thesection.}{1em}{} % Change the look of the section titles
\titleformat{\subsection}[block]{\large}{\thesubsection.}{1em}{} % Change the look of the section titles

\usepackage{fancyhdr} % Headers and footers
\pagestyle{fancy} % All pages have headers and footers
\fancyhead{} % Blank out the default header
\fancyfoot{} % Blank out the default footer
\fancyhead[C]{ECE 464 Final $\bullet$ March 2022} % Custom header text
\fancyfoot[RO,LE]{\thepage} % Custom footer text

\usepackage{titling} % Customizing the title section

\usepackage{hyperref} % For hyperlinks in the PDF

\newcommand{\newp}{\newline\indent}

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\setlength{\droptitle}{-4\baselineskip} % Move the title up

\pretitle{\begin{center}\Huge\bfseries} % Article title formatting
\posttitle{\end{center}} % Article title closing formatting
\title{Task Offloading to the Cloud for Embedded Devices} % Article title
\author{%
\textsc{Joseph Rubin} \\[1ex] % Your name
\normalsize Princeton University \\ % Your institution
\normalsize \href{mailto:jmrubin@princeton.edu}{jmrubin@princeton.edu} % Your email address
%\and % Uncomment if 2 authors are required, duplicate these 4 lines if more
%\textsc{Jane Smith}\thanks{Corresponding author} \\[1ex] % Second author's name
%\normalsize University of Utah \\ % Second author's institution
%\normalsize \href{mailto:jane@smith.com}{jane@smith.com} % Second author's email address
}
\date{\today} % Leave empty to omit a date
\renewcommand{\maketitlehookd}{%
\begin{abstract}
\noindent The number of real time embedded systems has increased following the IoT boom.
Real time embedded systems need to complete tasks before their deadlines expire, even with limited resources.
While offloading to higher-end machines in the local network is possible, it's often easier to set up a service running on the cloud to perform some of the work.
Deciding exactly which tasks to offload and when to do it is of principal importance, since cloud overhead (in terms of running time and the power consumption required to make network requests) must be considered.
In this paper, we use a challenge-based model for tasks based on Proof of Work to run experiments on novel algorithms to determine what methods perform well to determine how to offload tasks.
To do so, we create a simple simulation environment to help us collect data.
\end{abstract}
}

%----------------------------------------------------------------------------------------

\begin{document}

% Print the title
\maketitle

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\section{Introduction}

\lettrine[nindent=0em,lines=3]{C}{areful} design of an embedded system takes care to reduce power consumption and cost of materials.
Systems with real time-constraints, however, must complete tasks fast enough to meet their deadlines, even while using limited resources.
With today's availability of advanced cloud infrastructure an embedded device that is connected to the internet can offload data to be processed on a server and even retrieve the results.
The accelerating growth of IoT (Internet of Things)~\cite{7750968} means that many devices are indeed online and thus might benefit from off-device task assistance.
\newp This isn't a free lunch, though.
Using a network stack (such as WiFi) consumes power~\cite{7545919}, and running data through a network (especially when the device needs a response before it can continue) may be slow.
Thus the system will only benefit from offloading tasks that are sufficiently time or energy consuming to be worth the overhead of the network request.
Furthermore, the system may decide algorithmically how to batch tasks together before sending them to the cloud to minimize the overhead.
\newp In this report, I have created a simple real time operating system to simulate an embedded device with many deadline-bound periodic tasks running simultaneously.
I've produced a method of creating fake tasks with variable processing time requirements (based on proof of work) and modeled the power and time consumption of using the network stack to send the tasks to a cloud service.
Lastly, I've presented my findings as a result of running dozens of simulations with my proposed system and algorithms.
\newp In Section II I will discuss the papers in the field, in Section III I will provide explanations for concepts used throughout the report, in Section IV I will describe the motivation behind our methodology, and in Section V I will provide the technical details of our methodology.

\section{Related Work}

%------------------------------------------------

\section{Background}

\subsection{RTOS}

Real time systems collect information about the physical world from sensors and use that data to take action or report on their surroundings.
A Real-Time Operating System (RTOS) executes tasks on a system where data processing has strict time constraints (deadlines).
\newp In this report, we consider a fictional RTOS with strict deadlines.

\subsection{The Cloud}

Cloud providers such as AWS (used in this report) provide Infrastructure as a Service (IaaS) to their clients, allowing users to rent compute and storage media hosted in data centers across the world.
Choosing the closest data center will have large impact on our data collection because network propogation delay can be on the order of milliseconds or even seconds.
\newp Atop IaaS is Function as a service (FaaS); these offerings allow users to write a cloud service without choosing the physical characteristics of the hardware that it will be running on.
In this report, we use AWS Lambda (a FaaS) to create a low-latency endpoint for our embedded system to interact with.

\subsection{PoW}

Proof of Work is a system for verifying that computational expendiatures have been made by some party.
For this report, we use it as a simple way to model tasks with predictable runtimes.
\newp See section IV for more information about our use of PoW in this report.

%------------------------------------------------

\section{Motivation}

There are many types of embedded devices and they are used for a wide variety of purposes.
Thus instead of trying to model a particular device, we attempt to simulate the core similarities between all of the systems.
Namely
\begin{itemize}
  \item an RTOS with the ability to run multiple threads of execution in parallel.
  \item threads capable of offloading tasks to the cloud.
  \item tasks represented as PoW challenges.
  \item a cloud service that can complete tasks and return their results.
\end{itemize}

Simulation is a valuable technique for gathering metrics on complex systems.
For example, to understand how congestion will affect a network, it is common to simulate network topologies using software such as \texttt{Mininet}.
We can create a virtual setup of routers and switches and run simulated packets through the system.
In this way, many simulations can be run at once or in quick succession.
\newp This report follows a similar approach.
Rather than program a microcontroller and build a network-connected device, we can program a computer to behave like one.

%------------------------------------------------

\section{Methodology}

\subsection{Proposed Approach}
To that end, we programmed a basic real time OS in C++ (in order to use its low-level facilities to create threads, them being most similar to what would be available on an embedded system).
We didn't use processes because we want to ensure we only use one CPU core, and threads more closely resemble those created by hardware interupts in real devices (since they enable shared memory).
Our OS makes it easy to run periodic tasks in parallel and tracks when deadlines are not met (and by how much the deadlines were exceeded).

Each task is a PoW challenge $C_{n,p}$ with a solution set $S_{n,p}$ where
\[
  S_{n,p} = \{ k \in \mathbb{N} : \textnormal{SHA-256}(p || k)[0:n] = 0^n \}
\]

In other words, an $n$,$p$-challenge solution is a natural number whose SHA-256 output when appended to the challenge text $p$ starts with $n$ zero bits.
A solution is difficult to compute (and modulating $n$ will determine exactly how difficult) but easy to verify.
\newp The probability of finding a correct solution is $2^{-n}$, thus on average it will take $2^n$ guesses.
In order to make the time it takes to complete a task more predictable, we may decide to forgo a challenge of size $n$ and instead use $2^{n-(n/q)}$ challenges of size $n/q$ which will also take an expected $2^{n-(n/q)} \cdot 2^{n/q} = 2^n$ guesses.
\newp The device has the capability to solve these challenges itself, but it may also choose to send tasks in batches to the cloud service in JSON format:

\begin{verbatim}
{
  "tasks": [
    {"n": 10, "p": "sahfowebfsadfjwq"},
    {"n": 14, "p": "qerhbvcohaerjhfd"},
    {"n": 4,  "p": "pnosnvahiouhffwe"},
  ]
}
\end{verbatim}

The operating system will fork a new thread to complete each task and collect metrics about CPU usage and task completion.
\newp The cloud service is programmed in TypeScript on AWS lambda and will have access to increased resources.
It will solve all challenges sent in a single invocation and return the results over the network.
    
\subsection{Flowchart}

\begin{center}
  \includegraphics[scale=0.75]{flowchart.png}
\end{center}

The flowchart above details the orginization of the C++ program.
The main object is the \texttt{rtos} which is created with a list of task periods.

The \texttt{rtos} constructs a \texttt{task\_manager} for each period.
Each \texttt{task\_manager} launches a new thread to manage its tasks.

The responsibility of the thread running under a \texttt{task\_manager} is to continually launch a thread which delays until the next period begins and tries to solve a PoW challenge.

The \texttt{task manager} waits until each of its child threads finish in turn, and the \texttt{rtos} waits until all of its \texttt{task\_manager}s are done.

\subsection{Algorithms}

Consider an algorithm that does not batch tasks but instead offloads one task at a time to the cloud service.
For an $n$,$p$-challenge, there is some threshold $n>t$ where the overhead of sending the task to the network will be outweighed by the time savings.

But there is also a power draw associated with the network request.
If there are multiple ways to offload tasks that meet our timing constraints, we should choose the strategy that minimizes the total number of tasks offloaded.

\subsection{Experimental Results}

Of primary interest is discovering the best tasks to offload to the cloud in order to minimize -

Data\footnotemark was collected in four simulations.
For each, we control the tasks to be run in periods of $500$, $1000$, and $3000$.
We simulate offloading by allowing the task to complete a $16,p$-challenge instead of the usual $40,p$-challenge.

In the first simulation, we did not offload any tasks.
Because of the challenge difficulty, almost no tasks completed on time.
Only the first two tasks in the $3000$ period finished on time.

In the second simulation, we offloaded tasks for the $500$ period.
All 40 tasks in the $500$ period finished on time, but this did not take the load off of the other tasks too much; indeed only one addition task from the $3000$ period finished on time.

In the third simulation, we offloaded tasks for the $1000$ period.
All $20$ tasks in the $1000$ period finished on time, but no more tasks from the other periods completed on time compared to the first simulation.

In the fourth simulation, we offloaded tasks for the $3000$ period.
All $6$ tasks in the $3000$ period finished on time (an increase of $four$ tasks from the first simulation), but no other tasks finished on time.

\begin{center}
\begin{tabular}{ l l l l l }
          & no offl. & offl. 500 & offl. 1000 & offl. 3000 \\
500 late  & 1,626,249  & 0         & 1,418,425    & 1,648,492    \\
1000 late & 400,664   & 494,212    & 0          & 323,041     \\
3000 late & 24,103    & 14,027     & 4,943       & 0          \\
total late& 2,051,016  & 508,239    & 1,423,368    & 1,971,533 \\
late * no. offl& n/a & 20,329,560 & 28,467,360 & 11,829,198
\end{tabular}
\end{center}

\footnotetext{
  Raw data can be found in the appendix.
}

\subsection{Discussion and Limitations}

From a cursory glance at our data, it seems as though offloading tasks to the cloud does not appear to take the load off the processor enough to make other tasks complete on time.

If we look at how late the delinquent tasks are, it is clear that offloading the $40$ tasks in the $500$ period minimizes the total lateness.

However, when using the strategy of offloading the tasks in the $3000$ period, we only had to offload $6$ tasks.
This is $6.66$ times less than the $40$ tasks in the $500$ period.
Thus on a per-task basis, the offloading $3000$ strategy is more efficient.
This is shown by the last row in the table below.

We must be cautious to read too much into this experiment, though.
We only tested one configuration of task periods and challenge sizes.
Also, we didn't apply any strategies that would start tasks early if a previous task completed on time.

\subsection{Conclusions}

We learn from our experiment that offloading the shortest period tasks minimizes total lateness (because the most tasks are offloaded), but we can achieve the most efficient results (in terms of number of tasks offloaded) by offloading the tasks with the longest period.

Future work would look at more complicated offloading strategies that batch tasks and offload to a real cloud service.

The code for this report is located at https://github.com/josephrubin/464-final

%----------------------------------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------

\nocite{*}
\bibliographystyle{IEEEtranS}
\bibliography{references}

\newpage
\newpage

\section{Appendix}

Here's the raw data.
Results indicate how late each task completion was, where a 0 indicates that the task completed on time:

challenge size = 5, periods = {500, 1000, 3000}, no offloading

Thread of period 500 finished, [40] results: 3002 5103 6791 7688 14049 16768 16753 17410 17687 20203 23209 24742 33319 33023 33179 33837 34097 38824 39950 39560 40489 41635 44357 44305 47552 50560 52446 52083 53077 57874 61922 62663 66090 66712 67094 67510 69681 72416 73849 74740
Thread of period 1000 finished, [20] results: 1791 4440 5281 5580 11204 15576 15845 17979 18968 19771 22117 22531 22167 22389 24518 32927 32309 34031 35785 35455 
Thread of period 3000 finished, [6] results: 0 0 124 2230 10273 11476

challenge size = 5, periods = {500, 1000, 3000}, offloading tasks for period 500

Thread of period 500 finished, [40] results: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
Thread of period 1000 finished, [20] results: 1264 2179 9764 19225 20658 20082 21834 21286 24598 27052 27223 28955 28300 32123 32954 34510 34117 35284 36553 36251
Thread of period 3000 finished, [6] results: 0 0 0 965 5908 7154

challenge size = 5, periods = {500, 1000, 3000}, offloading tasks for period 1000

Thread of period 500 finished, [40] results: 2144 4583 10048 10625 12050 13796 16134 17944 24667 24518 24649 25145 25448 28774 29659 29757 29557 30256 31588 32930 33903 35875 35855 35790 38979 41533 43462 43082 44078 48897 52947 53666 57092 57704 58095 58510 60700 63401 64850 65734
Thread of period 1000 finished, [20] results: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
Thread of period 3000 finished, [6] results: 0 0 2289 660 1458 536

challenge size = 5, periods = {500, 1000, 3000}, offloading tasks for period 3000

Thread of period 500 finished, [40] results: 2490 4015 6497 7093 8754 16708 17262 19240 20860 22446 23583 32678 32520 32689 33367 33603 38599 39753 40276 41927 42918 44867 44850 44806 47967 50557 52431 52084 53078 57864 61924 62674 66107 66710 67092 67512 69704 72390 73857 74740 
Thread of period 1000 finished, [20] results: 88 3239 4036 4584 9421 11136 11720 11672 13063 15829 16529 19224 19567 19168 19422 21667 30152 30331 30081 32112
Thread of period 3000 finished, [6] results: 0 0 0 0 0 0

%----------------------------------------------------------------------------------------

\end{document}
